[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tweet Analytics for Disaster & Calamity Management",
    "section": "",
    "text": "In addressing the dynamic challenges of disasters, our project, Tweet Analytics for Disaster & Calamity Management, strives to generate vital and reliable information as a valuable resource for those in need. Our focus involves discerning the distinction between fake and genuine tweets pertaining to disasters like forest fires, earthquakes, and floods. Employing binary classification and advanced models, we analyze real-time data to deliver life-saving insights through an integrated GUI. This initiative aims to provide actionable and trustworthy information for effective disaster management and response."
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Tweet Analytics for Disaster & Calamity Management",
    "section": "",
    "text": "In addressing the dynamic challenges of disasters, our project, Tweet Analytics for Disaster & Calamity Management, strives to generate vital and reliable information as a valuable resource for those in need. Our focus involves discerning the distinction between fake and genuine tweets pertaining to disasters like forest fires, earthquakes, and floods. Employing binary classification and advanced models, we analyze real-time data to deliver life-saving insights through an integrated GUI. This initiative aims to provide actionable and trustworthy information for effective disaster management and response."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Tweet Analytics for Disaster & Calamity Management",
    "section": "Introduction",
    "text": "Introduction\nIn the face of the ever-changing landscape of disasters and calamities, effective and timely management requires innovative solutions that harness the power of social media. Our project addresses the inherent challenges in traditional methods of information gathering during crises, focusing on Twitter as a valuable source of real-time data. With a proactive approach, we delve into the classification of tweets using techniques like tokenisation to find keywords for disaster and non-disaster-related content and advanced models like BERT and LSTM. By extracting data from topics such as forest fires and earthquakes using Tweepy, we aim to contribute to a robust disaster management strategy. The upcoming integration of a user-friendly GUI interface will facilitate the dissemination of our predictions and outputs, ensuring a comprehensive and accessible tool for emergency responders and disaster management professionals."
  },
  {
    "objectID": "index.html#work-flow",
    "href": "index.html#work-flow",
    "title": "Tweet Analytics for Disaster & Calamity Management",
    "section": "Work Flow",
    "text": "Work Flow\n\nFake Tweet Classification\nWe performed classification of tweets into disaster-related and non-disaster-related categories using keyword count, character count, and word count distribution. This analysis served to enhance our understanding of the content and context of tweets during calamities.\n\n\nBERT Model Implementation\nDespite encountering resource constraints, we embarked on implementing a BERT (Bidirectional Encoder Representations from Transformers) model, achieving an accuracy of approximately 82% after three epochs. The BERT model is renowned for its contextual understanding and representation capabilities.\n\n\nLSTM Model Exploration\nIn response to resource limitations, we explored the implementation of an LSTM (Long Short-Term Memory) model, achieving a commendable accuracy of around 78%. The LSTM model, a type of recurrent neural network, demonstrated its effectiveness in capturing sequential dependencies in the data.\n\n\nData Extraction from Tweepy\nWe leveraged the Tweepy API to extract data pertaining to specific disaster topics, such as forest fires, floods, earthquakes, and hurricanes. This enabled us to gather real-time information and enhance the comprehensiveness of our analysis.\n\n\nGUI Interface Presentation\nAs part of our future work, we intend to present our predictions and outputs through an intuitive GUI (Graphical User Interface) interface. This interface will facilitate user-friendly access to our analytical insights, promoting effective decision-making in disaster and calamity management."
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "Tweet Analytics for Disaster & Calamity Management",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, our project endeavors to provide a robust and comprehensive solution to the challenges posed by dynamic disasters and calamities. Through a combination of advanced models, insightful visualizations, and real-time data extraction, we aim to contribute to proactive and informed disaster management strategies."
  },
  {
    "objectID": "index.html#approach",
    "href": "index.html#approach",
    "title": "Tweet Analytics for Disaster & Calamity Management",
    "section": "Approach",
    "text": "Approach\n\nFake Tweet Classification\nWe performed classification of tweets into disaster-related and non-disaster-related categories using keyword count, character count, and word count distribution. This analysis served to enhance our understanding of the content and context of tweets during calamities. To gain deeper insights, we generated word clouds for both disaster and non-disaster tweets, providing a visual representation of the most frequent words in each category.\n\n\nTokenization\nWe tokenized the text data with the help of Tokenizer() from Keras library in python with vocabulary size of 1000. After tokenization, we had a list of sequences where each sequence represents the texts converted into a sequence of integers .\n\n\nBERT Model Implementation\nWe tried to implement a BERT-based binary classification model, achieving an accuracy of approximately 82% after three epochs. The BERT model is renowned for its contextual understanding and representation capabilities. Due to lack of computational resources we had to abort the model and moved ahead with a different approach.\n\n\nLSTM Model Exploration\nIn response to resource limitations, we explored the implementation of an LSTM (Long Short-Term Memory) model, achieving a commendable accuracy of around 78%. The LSTM model, a type of recurrent neural network, demonstrated its effectiveness in capturing sequential dependencies in the data.\n\n\nData Extraction from Tweepy\nWe leveraged the Tweepy API to extract data pertaining to specific disaster topics, such as forest fires, floods, earthquakes, and hurricanes. This enabled us to gather real-time information and enhance the comprehensiveness of our analysis.\n\n\nGUI Interface Presentation\nAs part of our future work, we intend to present our predictions and outputs through an intuitive GUI (Graphical User Interface) interface. This interface will facilitate user-friendly access to our analytical insights, promoting effective decision-making in disaster and calamity management."
  },
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Tweet Analytics for Disaster & Calamity Management",
    "section": "",
    "text": "Unleashing the Power of Twitter to strengthen Disaster Response and safeguard public safety during Critical Events."
  },
  {
    "objectID": "proposal.html#high-level-goal",
    "href": "proposal.html#high-level-goal",
    "title": "Tweet Analytics for Disaster & Calamity Management",
    "section": "",
    "text": "Unleashing the Power of Twitter to strengthen Disaster Response and safeguard public safety during Critical Events."
  },
  {
    "objectID": "proposal.html#goal-description-motivation",
    "href": "proposal.html#goal-description-motivation",
    "title": "Tweet Analytics for Disaster & Calamity Management",
    "section": "Goal Description & Motivation:",
    "text": "Goal Description & Motivation:\nSocial media platforms like Twitter provide an unprecedented opportunity to access a vast amount of information and gain insights directly from those affected by disasters. By analyzing the conversations and trends on Twitter during such situations, the project aims to uncover patterns, validate the truthfulness of information, improve situational awareness, and ultimately contribute to the resilience and safety of individuals and communities faced with critical events.\nThe major motivation behind the proposed project is to save lives and improve public safety by harnessing the power of Twitter data to access real-time information, understand & validate public sentiment."
  },
  {
    "objectID": "proposal.html#dataset",
    "href": "proposal.html#dataset",
    "title": "Tweet Analytics for Disaster & Calamity Management",
    "section": "Dataset",
    "text": "Dataset\nThis dataset has been taken from Natural Language Processing with Disaster Tweets competition. Twitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they're observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\nThere are two main files associated with this dataset:\n\ntrain.csv - credibility ratings and reasoning entered by turkers** for each topic. Each row has following tab separated fields:\nData Glimpse\n\n\n\n\n\n\n\nid\n\n\nkeyword\n\n\nlocation\n\n\ntext\n\n\ntarget\n\n\n\n\n\n\n66\n\n\nablaze\n\n\nGREENSBORO,NORTH CAROLINA\n\n\nHow the West was burned: Thousands of wildfire…\n\n\n1\n\n\n\n\n\n\n\nData Description\n\n\n\n\n\n\n\n\nColumn Name\nData Type\nDescription\n\n\n\n\nid\nInteger\nA unique identifier for each tweet\n\n\ntext\nList of strings\nIt contains the text of the tweet\n\n\nlocation\nList of strings\nThe location the tweet was sent from (may be blank)\n\n\nkeyword\nList of Strings\nA particular keyword from the tweet (may be blank)\n\n\ntarget\nInteger\nin train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n\n\n\nThis data set will be used to train the model on the credibility of the tweet\ntest.csv - tweets corresponding to each topic fetched using the search API. Each row has following tab separated fields:\nData Description\n\n\n\n\n\n\n\n\nColumn Name\nData Type\nDescription\n\n\n\n\nid\nInteger\nA unique identifier for each tweet\n\n\ntext\nList of strings\nIt contains the text of the tweet\n\n\nlocation\nList of strings\nThe location the tweet was sent from (may be blank)\n\n\nkeyword\nList of strings\nA particular keyword from the tweet (may be blank)\n\n\n\nThis dataset will be used as an index to fetch and test specific types of disaster tweets (like earthquake).\n\nFor streaming API, we will be using tweepy, a python package meand for extracting real-time tweets using keywords. It will be authenticated using a developer account from the v2 API endpoint. This endpoint will serve on a free tier to fetch and consume streaming data from twitter (https://developer.twitter.com/en/docs/tutorials/consuming-streaming-data). The individual data streamed by this API are JSON encoded, and fall into the following types:\n\nTweets: Individual Tweet JSON objects\nKeep-alive Signals: Carriage returns to prevent your connection from timing out\nSystem messages: E.g. notification of a force disconnect. Note that the actual disconnection is accomplished via normal HTTP protocols, rather than through the message itself. In some cases the disconnect system message may not arrive, making it critical that you monitor the keep-alive signal (see below for more information).\n\nWe will be utilizing the Tweet JSON (shown below) to analyze and classify them into real or fake. This can be particularly useful for disaster management departments, as the analysis of these tweets can help extract crucial information, such as the geolocation of the tweet, which can be used to send help to those in need.\n\n\nExpand to reveal Tweet JSON output\n{\n  \"created_at\": \"Thu Apr 06 15:24:15 +0000 2017\",\n  \"id_str\": \"850006245121695744\",\n  \"text\": \"1\\/ Today we\\u2019re sharing our vision for the future of the Twitter API platform!\\nhttps:\\/\\/t.co\\/XweGngmxlP\",\n  \"user\": {\n    \"id\": 2244994945,\n    \"name\": \"Twitter Dev\",\n    \"screen_name\": \"TwitterDev\",\n    \"location\": \"Internet\",\n    \"url\": \"https:\\/\\/dev.twitter.com\\/\",\n    \"description\": \"Your official source for Twitter Platform news, updates & events. Need technical help? Visit https:\\/\\/twittercommunity.com\\/ \\u2328\\ufe0f #TapIntoTwitter\"\n  },\n  \"place\": {   \n  },\n  \"entities\": {\n    \"hashtags\": [      \n    ],\n    \"urls\": [\n      {\n        \"url\": \"https:\\/\\/t.co\\/XweGngmxlP\",\n        \"unwound\": {\n          \"url\": \"https:\\/\\/cards.twitter.com\\/cards\\/18ce53wgo4h\\/3xo1c\",\n          \"title\": \"Building the Future of the Twitter API Platform\"\n        }\n      }\n    ],\n    \"user_mentions\": [     \n    ]\n  }\n}\n\n\n\n\n\n\n\n\n\n\nKey Name\nData Type\nDescription\n\n\n\n\ncreated_at\nString\nUTC time when this Tweet was created.\n\n\nid_str\nString\nThe string representation of the unique identifier for this Tweet.\n\n\ntext\nString\nThe actual UTF-8 text of the status update.\n\n\nuser | User object | The user who posted this Tweet. See User data dictionary for complete list of attributes.\n\n\n\n\nplace\nPlaces Object\nWhen present, indicates that the tweet is associated (but not necessarily originating from) a Place.\n\n\nentities\nEntities Object\nEntities which have been parsed out of the text of the Tweet. Additionally see Entities in Twitter Objects. Example:\n{ “hashtags”:[],\n“urls”:[],\n“user_mentions”:[],\n“media”:[],\n“symbols”:[]\n“polls”:[] }\n\n\n\n\n\nThe combination of these two sources provides a rich and comprehensive data set for your application. The credibility annotations in the CREDBANK data can aid in the fake/real classification, while the academictwitteR package can help in collecting real-time data for ongoing analysis."
  },
  {
    "objectID": "proposal.html#reason-for-choosing-this-source",
    "href": "proposal.html#reason-for-choosing-this-source",
    "title": "Tweet Analytics for Disaster & Calamity Management",
    "section": "Reason For Choosing This Source",
    "text": "Reason For Choosing This Source\nThese dataset’s not only provide a large volume of data but also offer unique features that align well with our project’s objectives. The combination of historical data from CREDBANK and real-time data from academictwitteR can provide a robust foundation for the tweet analysis application."
  },
  {
    "objectID": "proposal.html#problem-statement",
    "href": "proposal.html#problem-statement",
    "title": "Tweet Analytics for Disaster & Calamity Management",
    "section": "Problem Statement",
    "text": "Problem Statement\nThe dynamic nature of disasters and calamities demands a proactive approach to information gathering and analysis. Traditional methods often fail to adapt quickly to evolving situations, hindering the ability of emergency responders to anticipate needs and allocate resources effectively. Furthermore, ensuring the reliability and credibility of the information shared on Twitter during these events is crucial, as misinformation and rumors can lead to panic and confusion among the affected population."
  },
  {
    "objectID": "proposal.html#analysis-plan",
    "href": "proposal.html#analysis-plan",
    "title": "Tweet Analytics for Disaster & Calamity Management",
    "section": "Analysis Plan",
    "text": "Analysis Plan\nFor the project, we are going to split into weekly milestones and attack the project in a piece by piece method. The goal is to build on the project an deploy a fully functional real-time classifier from the Twitter API. The plan is split into 6 weeks, with each of the key milestones described as follows:\nWeek 1:\nAvikal and Himanshu:\n\nDataset Exploration\n\nExtract characteristics from the dataset, like UserID, Message Content, Location, etc.\nConvert to Binary Classes.\n\n\nShakir and Poojitha: - Data Preprocessing\n\nNull value and Duplicate value treatment.\nText Cleaning (removing special characters, stop words).\nSplit dataset into training and testing.\n\nVisalakshi, Himanshu and Avikal: - Feature Extraction\n\nTokenization.\nStemming.\nLemmatization.\n\nAll members: - Classification\n\nTrain and validate the model.\nTest on new tweets.\n\nWeek 2:\n\nAvikal, Visalakshi and Shakir: Extract entities from the tweet that include information about geo-location, disaster-type, severity, time elapsed, etc. using NER techniques.\nHimanshu and Poojitha: Link the entities to tweets and save the information.\n\nWeek 3:\n\nVisalakshi and Avikal: Use python Tweepy to extract information and setup data flow.\nShakir: Define filtering parameters to separate disaster-related tweets.\nPoojitha and Himanshu: Process and store the incoming tweet data along with the extracted results from the tweet data.\n\nWeek 4:\n\nEveryone: Visualize the extracted output in a GUI. This will include showing geographic locations of the affected regions.\nEveryone: Add a dashboard to this GUI that provides better analysis of the extracted information.\n\nWeek 5:\n\nAvika;, Shakir and Visalakshi: Create an interface for the analysis dashboard.\nVisalakshi and Himanshu: Use maps to indicate affected locations.\nPoojitha: Generate intelligent analytics on the type of disaster, time elapsed, severity, etc.\n\nWeek 6:\n\nEveryone: Deploy the GUI and the data.\nEveryone: Make changes according to feedback.\nEveryone: Make the final presentation for showcase.\n\n\n\n\n\n\n\nNote:\n\n\n\nThese are the planned approaches, and we intend to explore and solve the problem statement which we came up with. Parts of our approach might change in the final project."
  },
  {
    "objectID": "proposal.html#plan-of-attack",
    "href": "proposal.html#plan-of-attack",
    "title": "Tweet Analytics for Disaster & Calamity Management",
    "section": "Plan of Attack",
    "text": "Plan of Attack\n\n\n\n\n\n\n\nWeek\nWeekly Task\n\n\n\n\nWeek 1\nProposal.\n\n\nWeek 2\nData Exploration and Preprocessing.\n\n\nWeek 3 - Week 4\nWorking on setting up the API and Extracting Feature using methods like Tokenization, lemmatization etc. and train Models and perform prediction.\n\n\nWeek 5\nVisualize the output by working on developing a GUI.\n\n\nWeek 6\nFinalizing the project and complete preparing a presentation.\n\n\n\n** We have added respective members for individual tasks in the Analysis Plan. We have removed Members Responsible Column from the table."
  },
  {
    "objectID": "proposal.html#repo-organization",
    "href": "proposal.html#repo-organization",
    "title": "Tweet Analytics for Disaster & Calamity Management",
    "section": "Repo Organization",
    "text": "Repo Organization\n\n.github: This directory contain files related to GitHub, such as workflows, issue templates, or other configurations.\n. _extra: Contains code, notes and other files used during experimentation. Contents of this folder is not a part of the final output.\n_freeze: The folder created to store files generated during project render.\nanalysis/: This folder contains the analysis scripts used to generate output for the project outline.\n\nREADME.md describes the steps to run and generate the results using scripts\nOther code files are added to review and understand the source of the output\nRelevant datasets used for the scripts are under the data/ folder in the main directory.\n\ndata/: This folder contains data files or datasets that are used in the project.\n\nREADME.md : A readme file that describes the datasets in more detail.\n\nimages: This folder contains image files that are used in the project, such as illustrations, diagrams, or other visual assets.\n.gitignore: This file specifies which files or directories should be ignored by version control.\nREADME.md: This file usually contains documentation or information about the project. It’s often the first thing someone reads when they visit the project repository.\n_quarto.yml: This is likely a configuration file\nabout.qmd : This quarto document contains the information about team members.\nindex.qmd : This quarto document contains the approach and analysis and results of the project.\npresentation.qmd : It contains the slides for the presentation.\nproposal.qmd : This quarto documents has the proposal of the project.\nproject-final.Rproj : This is an RStudio project file, which helps organize R-related files and settings for the project."
  }
]