{
  "hash": "9a4a4d91c89d288080716b7295d90e07",
  "result": {
    "markdown": "---\ntitle: \"Tweet Analytics for Disaster and Calamity Management\"\nsubtitle: \"INFO 523 - Fall 2023 - Project Final\"\nauthor: \"The Algo-Rythms Team : Visalakshi Prakash Iyer, Himanshu Nimbarte, Shakir Ahmed, Avikal Singh, Poojitha Pasala\"\ntitle-slide-attributes:\n  data-background-image: images/watercolour_sys02_img34_teacup-ocean.jpg\n  data-background-size: stretch\n  data-background-opacity: \"0.7\"\n  data-slide-number: none\nformat:\n  revealjs:\n    theme:  ['data/customtheming.scss']\n    transition: slide\n    background-transition: fade\n    logo: images\\twitter_logo.png\n    footer: \"[ðŸ•Š The Algo-Rhythms](https://info-523-exercises.github.io/project-final-The-Algo-Rhythms/)\"\n    scrollable: true\n    style: |\n      body {\n        font-size: 12px; /* Set your desired font size here */\n      }\n  \neditor: visual\nexecute:\n  echo: false\n---\n\n\n\n\n\n\n\n\n\n## Introduction: {.smaller}\n\n-   In times of disasters, individuals frequently turn to social media platforms to communicate information regarding required aid or incidents.\n\n-   Twitter provides an unprecedented opportunity to access a vast amount of information and gain insights directly from those affected by disasters. Harnessing of Twitter's potential to is crucial to enhance disaster response and ensure public safety in critical situations.\n\n## Goal and problem statement: {.smaller}\n\n-   **Goal-** Conduct classification on tweets during crisis, validate tweets, and deliver accurate information to disaster management teams, thereby contributing to the smooth functioning of rescue operations and boosting the safety and resilience of communities during critical events.\n\n-   **Problem statement-** In the face of dynamic disasters, a proactive approach to information is essential. Traditional methods can lag, hampering swift responses and resource allocation. Validating Twitter-shared information is critical to prevent panic and confusion among affected communities.\n\n## Data: {.smaller}\n\n-   Real-time tweets fetched from twitter based on type of disaster (hashtag) and location. (For testing)\n\n-   Fake Tweet Classification dataset from kaggle is used for classification of tweets as real or fake. (For training)\n\n## Execution of plan: {.smaller}\n\n1.  **Created a new streaming pipeline-** Used NTscraper and tweepy package to create a Nitter object which uses Beautiful soup in the back end to fetch tweet details such as \"text\", \"username\" and statistics such as \"likes\",\"comments\" and \"retweets\".\n2.  **Model building-** Used LSTM model to build a fake news classification model and give us the relevant and important details corresponding to disaster management.\n3.  **Model training-** Used Kaggle dataset to perform model training and get accuracy scores.\n4.  **Model testing-** Use real time data based on hastag- \"#forestfire\" and location set to near \"USA\".\n5.  **GUI creation-** Integrate a search bar for input of hastag, after which the model fetches the data and performs classification. The classified real tweets are displayed with tweet link, location (if available) and the user.\n\n## Results {.smaller}\n\n\\<\\<\\<INSERT IMAGES LATER\\>\\>\\>\n\n## Live Demo: {.smaller}\n\nNow, we are going to demonstrate our project in action.\n\n\\<\\<\\<INSERT LINK HERE\\>\\>\\>\n\n## Conclusion: {.smaller}\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}