---
title: "Tweet Analytics for Disaster & Calamity Management"
subtitle: "Proposal"
format: html
editor: visual
---

```{r load-packages, echo=FALSE, message=FALSE, warning=FALSE}

if(!require(pacman))
  install.packages("pacman")

pacman::p_load(here, tidyverse, dplyr,
               formattable)
```

## High Level Goal

Unleashing the Power of Twitter to strengthen Disaster Response and safeguard public safety during Critical Events.

## **Goal Description & Motivation:**

Social media platforms like Twitter provide an unprecedented opportunity to access a vast amount of information and gain insights directly from those affected by disasters. By analyzing the conversations and trends on Twitter during such situations, the project aims to uncover patterns, validate the truthfulness of information, improve situational awareness, and ultimately contribute to the resilience and safety of individuals and communities faced with critical events.\
The major motivation behind the proposed project is to save lives and improve public safety by harnessing the power of Twitter data to access real-time information, understand & validate public sentiment.

## Dataset

[The **CREDBANK** dataset](https://github.com/compsocial/CREDBANK-data) is a large-scale social media corpus. It consists of streaming tweets tracked over this period, topics in this tweet stream, topics classified as events or non-events, and events annotated with credibility ratings. The data is spread across four files, including a file containing more than 169 million streaming tweets and a file containing more than 62,000 topics. Each topic is represented by three terms, corresponding to the top three terms returned by running topic modeling (LDA) over the streaming tweets.

There are two main files associated with credibility annotations:

-   [cred_event_TurkRatings.data](https://s3-us-west-2.amazonaws.com/credbank/cred_event_TurkRatings.data) - credibility ratings and reasoning entered by turkers\*\* for each topic. Each row has following tab separated fields:

    **Data Glimpse**

    ```{r table-one, echo=FALSE, message = FALSE, warning=FALSE}

    tibble(topic_key = "louis_ebola_nurse-20141024_170629-20141024_181626",
           topic_terms = c("'louis', 'ebola', 'nurse'"),
           Ratings_list = c("'1', '-1', '2', '-2', '0', '2', '0', '....'"),
           Reasons_list = c("'Nurses union describes the procedures taken by nurse who now has Ebola from treating a patient.', '.....'")
          ) |>
      formattable()



    ```

    **Data Description**

    +--------------+-----------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | Column Name  | Data Type       | Description                                                                                                                                                                                                    |
    +==============+=================+================================================================================================================================================================================================================+
    | topic_key    | String          | This is a combination of the time_key and topic_terms from the Topic File. The entries in the Topic File which were marked as event (isEvent = 1) have credibility ratings in the credibility annotation file. |
    +--------------+-----------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | topic_terms  | List            | A list of 3 terms corresponding to the top 3 terms in each topic.                                                                                                                                              |
    +--------------+-----------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | Ratings_list | List            | A list of 30 credibility ratings entered by Turkers\*\*. The ratings are based on a 5-point Likert scale ranging from \[-2 to +2\]:                                                                            |
    |              |                 |                                                                                                                                                                                                                |
    |              |                 | -   \[-2\] Certainly Inaccurate                                                                                                                                                                                |
    |              |                 |                                                                                                                                                                                                                |
    |              |                 | -   \[-1\] Probably Inaccurate                                                                                                                                                                                 |
    |              |                 |                                                                                                                                                                                                                |
    |              |                 | -   \[0\] Uncertain (Doubtful)                                                                                                                                                                                 |
    |              |                 |                                                                                                                                                                                                                |
    |              |                 | -   \[+1\] Probably Accurate                                                                                                                                                                                   |
    |              |                 |                                                                                                                                                                                                                |
    |              |                 | -   \[+2\] Certainly Accurate                                                                                                                                                                                  |
    +--------------+-----------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | Reasons_List | List of Strings | A list of 30 reasons corresponding to the ratings entered by the Turkers\*\*.                                                                                                                                  |
    +--------------+-----------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

    This data set will be used to train the model on the credibility of the tweet

    *\*\* Turkers are remote "crowdworkers" hired to perform discrete tasks on-demand (as the rating the credibility by parsing each"*

-   [cred_event_SearhTweets.data](https://s3-us-west-2.amazonaws.com/credbank/cred_event_SearchTweets.data) - tweets corresponding to each topic fetched using the search API. Each row has following tab separated fields:

    **Data Glimpse**

    ```{r table-two, echo=FALSE, message = FALSE, warning=FALSE}

    tibble(topic_key = "louis_ebola_nurse-20141024_170629-20141024_181626",
           topic_terms = c("'louis', 'ebola', 'nurse'"),
           tweet_count = 13243,
           ListOf_tweetid_author_createdAt_tuple = "[('ID=522760301435830272', 'AUTHOR=iMhartyz', 'CreatedAt=2014-10-16 14:45:42'), ......]"
          ) |>
      formattable()
    ```

    **Data Description**

    +---------------------------------------+-----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | Column Name                           | Data Type             | Description                                                                                                                                                                                                    |
    +=======================================+=======================+================================================================================================================================================================================================================+
    | topic_key                             | String                | This is a combination of the time_key and topic_terms from the Topic File. The entries in the Topic File which were marked as event (isEvent = 1) have credibility ratings in the credibility annotation file. |
    +---------------------------------------+-----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | topic_terms                           | List                  | A list of 3 terms corresponding to the top 3 terms in each topic.                                                                                                                                              |
    +---------------------------------------+-----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | tweet_count                           | Long Int              | Total number of tweets returned by the search API for the topic_terms.                                                                                                                                         |
    +---------------------------------------+-----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | ListOf_tweetid_author_createdAt_tuple | List of Tuple Objects | A list of tuples, where each tuple contains three fields: tweet ID , tweet author , tweet creation date                                                                                                        |
    +---------------------------------------+-----------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

    This dataset will be used as an index to fetch and test specific types of disaster tweets (like earthquake).

For streaming API, we will be using tweepy, a python package meand for extracting real-time tweets using keywords. It will be authenticated using a developer account from the v2 API endpoint. This endpoint will serve on a free tier to fetch and consume streaming data from twitter (https://developer.twitter.com/en/docs/tutorials/consuming-streaming-data). The individual data streamed by this API are JSON encoded, and fall into the following types:

-   Tweets: Individual Tweet JSON objects

-   Keep-alive Signals: Carriage returns to prevent your connection from timing out

-   System messages: E.g. notification of a force disconnect. Note that the actual disconnection is accomplished via normal HTTP protocols, rather than through the message itself. In some cases the disconnect system message may not arrive, making it critical that you monitor the keep-alive signal (see below for more information).

    -   We will be utilizing the Tweet JSON (shown below) to analyze and classify them into real or fake. This can be particularly useful for disaster management departments, as the analysis of these tweets can help extract crucial information, such as the geolocation of the tweet, which can be used to send help to those in need.

        ```{r, eval=FALSE, collapse=TRUE, message=FALSE}
        #| code-fold: true
        #| code-summary: Expand to reveal Tweet JSON output

        {
          "created_at": "Thu Apr 06 15:24:15 +0000 2017",
          "id_str": "850006245121695744",
          "text": "1\/ Today we\u2019re sharing our vision for the future of the Twitter API platform!\nhttps:\/\/t.co\/XweGngmxlP",
          "user": {
            "id": 2244994945,
            "name": "Twitter Dev",
            "screen_name": "TwitterDev",
            "location": "Internet",
            "url": "https:\/\/dev.twitter.com\/",
            "description": "Your official source for Twitter Platform news, updates & events. Need technical help? Visit https:\/\/twittercommunity.com\/ \u2328\ufe0f #TapIntoTwitter"
          },
          "place": {   
          },
          "entities": {
            "hashtags": [      
            ],
            "urls": [
              {
                "url": "https:\/\/t.co\/XweGngmxlP",
                "unwound": {
                  "url": "https:\/\/cards.twitter.com\/cards\/18ce53wgo4h\/3xo1c",
                  "title": "Building the Future of the Twitter API Platform"
                }
              }
            ],
            "user_mentions": [     
            ]
          }
        }

        ```

        +------------------------------------------------------------------------------------------------------------------+-----------------+-----------------------------------------------------------------------------------------------------------------------+-------+
        | # Key Name                                                                                                       | # Data Type     | # Description                                                                                                         |       |
        |                                                                                                                  |                 |                                                                                                                       |       |
        | created_at                                                                                                       | String          | UTC time when this Tweet was created.                                                                                 |       |
        +------------------------------------------------------------------------------------------------------------------+-----------------+-----------------------------------------------------------------------------------------------------------------------+-------+
        |                                                                                                                  |                 |                                                                                                                       |       |
        +------------------------------------------------------------------------------------------------------------------+-----------------+-----------------------------------------------------------------------------------------------------------------------+-------+
        | id_str                                                                                                           | String          | The string representation of the unique identifier for this Tweet.                                                    |       |
        +------------------------------------------------------------------------------------------------------------------+-----------------+-----------------------------------------------------------------------------------------------------------------------+-------+
        | text                                                                                                             | String          | The actual UTF-8 text of the status update.                                                                           |       |
        +------------------------------------------------------------------------------------------------------------------+-----------------+-----------------------------------------------------------------------------------------------------------------------+-------+
        | user \| User object \| The user who posted this Tweet. See User data dictionary for complete list of attributes. |                 |                                                                                                                       |       |
        +------------------------------------------------------------------------------------------------------------------+-----------------+-----------------------------------------------------------------------------------------------------------------------+-------+
        | place                                                                                                            | Places Object   | When present, indicates that the tweet is associated (but not necessarily originating from) a Place.                  |       |
        +------------------------------------------------------------------------------------------------------------------+-----------------+-----------------------------------------------------------------------------------------------------------------------+-------+
        | entities                                                                                                         | Entities Object | Entities which have been parsed out of the text of the Tweet. Additionally see Entities in Twitter Objects. Example:\ |       |
        |                                                                                                                  |                 | { "hashtags":\[\],                                                                                                    |       |
        |                                                                                                                  |                 |                                                                                                                       |       |
        |                                                                                                                  |                 | "urls":\[\],                                                                                                          |       |
        |                                                                                                                  |                 |                                                                                                                       |       |
        |                                                                                                                  |                 | "user_mentions":\[\],                                                                                                 |       |
        |                                                                                                                  |                 |                                                                                                                       |       |
        |                                                                                                                  |                 | "media":\[\],                                                                                                         |       |
        |                                                                                                                  |                 |                                                                                                                       |       |
        |                                                                                                                  |                 | "symbols":\[\]                                                                                                        |       |
        |                                                                                                                  |                 |                                                                                                                       |       |
        |                                                                                                                  |                 | "polls":\[\] }                                                                                                        |       |
        +------------------------------------------------------------------------------------------------------------------+-----------------+-----------------------------------------------------------------------------------------------------------------------+-------+

The combination of these two sources provides a rich and comprehensive data set for your application. The credibility annotations in the CREDBANK data can aid in the fake/real classification, while the academictwitteR package can help in collecting real-time data for ongoing analysis.

## Reason For Choosing This Source

These dataset's not only provide a large volume of data but also offer unique features that align well with our project's objectives. The combination of historical data from `CREDBANK` and real-time data from `academictwitteR` can provide a robust foundation for the tweet analysis application.

## Problem Statement

The dynamic nature of disasters and calamities demands a proactive approach to information gathering and analysis. Traditional methods often fail to adapt quickly to evolving situations, hindering the ability of emergency responders to anticipate needs and allocate resources effectively. Furthermore, ensuring the reliability and credibility of the information shared on Twitter during these events is crucial, as misinformation and rumors can lead to panic and confusion among the affected population.

## Analysis Plan

For the project, we are going to split into weekly milestones and attack the project in a piece by piece method. The goal is to build on the project an deploy a fully functional real-time classifier from the Twitter API. The plan is split into 6 weeks, with each of the key milestones described as follows:

**Week 1:**

**Avikal and Himanshu:**

-   Dataset Exploration

    -   Extract characteristics from the dataset, like UserID, Message Content, Location, etc.

    -   Convert to Binary Classes.

**Shakir and Poojitha:** - Data Preprocessing

-   Null value and Duplicate value treatment.

-   Text Cleaning (removing special characters, stop words).

-   Split dataset into training and testing.

**Visalakshi, Himanshu and Avikal:** - Feature Extraction

-   Tokenization.

-   Stemming.

-   Lemmatization.

**All members:** - Classification

-   Train and validate the model.

-   Test on new tweets.

**Week 2:**

-   **Avikal, Visalakshi and Shakir:** Extract entities from the tweet that include information about geo-location, disaster-type, severity, time elapsed, etc. using NER techniques.

-   **Himanshu and Poojitha:** Link the entities to tweets and save the information.

**Week 3:**

-   **Visalakshi and Avikal:** Use python Tweepy to extract information and setup data flow.

-   **Shakir:** Define filtering parameters to separate disaster-related tweets.

-   **Poojitha and Himanshu:** Process and store the incoming tweet data along with the extracted results from the tweet data.

**Week 4:**

-   **Everyone:** Visualize the extracted output in a GUI. This will include showing geographic locations of the affected regions.

-   **Everyone:** Add a dashboard to this GUI that provides better analysis of the extracted information.

**Week 5:**

-   **Avika;, Shakir and Visalakshi:** Create an interface for the analysis dashboard.

-   **Visalakshi and Himanshu:** Use maps to indicate affected locations.

-   **Poojitha:** Generate intelligent analytics on the type of disaster, time elapsed, severity, etc.

**Week 6:**

-   **Everyone:** Deploy the GUI and the data.

-   **Everyone:** Make changes according to feedback.

-   **Everyone:** Make the final presentation for showcase.

::: callout-note
## Note:

These are the planned approaches, and we intend to explore and solve the problem statement which we came up with. Parts of our approach might change in the final project.
:::

## Plan of Attack

+---------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+
| Week                | Weekly Task                                                                                                                                       |
+=====================+===================================================================================================================================================+
| Week 1              | Proposal.                                                                                                                                         |
+---------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+
| Week 2              | Data Exploration and Preprocessing.                                                                                                               |
+---------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+
| Week 3 - Week 4     | Working on setting up the API and Extracting Feature using methods like Tokenization, lemmatization etc. and train Models and perform prediction. |
+---------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+
| Week 5              | Visualize the output by working on developing a GUI.                                                                                              |
+---------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+
| Week 6              | Finalizing the project and complete preparing a presentation.                                                                                     |
+---------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+

*\*\* We have added respective members for individual tasks in the Analysis Plan. We have removed Members Responsible Column from the table.*

## Repo Organization

-   `.github`: This directory contain files related to GitHub, such as workflows, issue templates, or other configurations.

-   `. _extra`: Contains code, notes and other files used during experimentation. Contents of this folder is not a part of the final output.

-   `_freeze`: The folder created to store files generated during project render.

-   `data/`: This folder contains data files or datasets that are used in the project.

    -   README.md : A readme file that describes the datasets in more detail.

-   `images`: This folder contains image files that are used in the project, such as illustrations, diagrams, or other visual assets.

-   `.gitignore`: This file specifies which files or directories should be ignored by version control.

-   `README.md`: This file usually contains documentation or information about the project. It's often the first thing someone reads when they visit the project repository.

-   `_quarto.yml`: This is likely a configuration file

-   `about.qmd` : This quarto document contains the information about team members.

-   `index.qmd` : This quarto document contains the approach and analysis and results of the project.

-   `presentation.qmd` : It contains the slides for the presentation.

-   `proposal.qmd` : This quarto documents has the proposal of the project.

-   `project-final.Rproj` : This is an RStudio project file, which helps organize R-related files and settings for the project.
